{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a model to data using **zeus**\n",
    "\n",
    "https://zeus-mcmc.readthedocs.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zeus\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('zeus version:', zeus.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The generative model\n",
    "\n",
    "Lets define a sinusoidal model with Gaussian uncorrelated measurement errors.\n",
    "\n",
    "$$ y = A \\sin \\Bigg[ 2\\pi \\Big(\\frac{t}{P}+t_{0}\\Big)\\Bigg] + B + \\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim \\mathcal{N}(0,\\sigma^{2})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sine_model(params, t):\n",
    "    A, B, P, t0 = params\n",
    "    return A * np.sin((t / P + t0) * 2 * np.pi) + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the model to generate some synthetic data/observations. To this end we first need to define the time instances in which we \"took\" our measurements as well as how many such measurements we obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Number of data points\n",
    "n_data = 50\n",
    "\n",
    "# time of observations\n",
    "t = np.random.uniform(0, 5, size=n_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to decide what are the \"true\" values of the parameters and generate the data assuming a measurement error (standard deviation) of $\\sigma = 1.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A=4.3, B=1.0, P=3.1, t=0.4\n",
    "params_true = [4.3, 1.0, 3.1, 0.4]\n",
    "\n",
    "yerr = 1.0\n",
    "y = np.random.normal(sine_model(params_true, t), yerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.errorbar(x=t, y=y, yerr=yerr, marker='o', ls=' ')\n",
    "plt.xlabel('t', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior, Likelihood and Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given some data $D$ and a model $\\mathcal{M}$ with parameters $\\theta$ we define the posterior distribution $\\mathcal{P}(\\theta)\\equiv P(\\theta | D, \\mathcal{M})$ using Bayes's rule:\n",
    "\n",
    "$$\\mathcal{P}(\\theta) = \\frac{\\mathcal{L}(\\theta) \\pi (\\theta)}{\\mathcal{Z}}$$\n",
    "\n",
    "where $\\mathcal{L}(\\theta) \\equiv P(D|\\theta, \\mathcal{M})$ is the likelihood function, $\\pi (\\theta) \\equiv P (\\theta | \\mathcal{M})$ is the prior distribution of the model parameters $\\theta$, and $\\mathcal{Z}\\equiv P(D|\\mathcal{M})$ is the, so called, Bayesian model evidence or marginal likelihood.\n",
    "\n",
    "\n",
    "Starting with the prior we can assume independend flat (uniform) priors such that\n",
    "\n",
    "$$ \\pi (\\theta) \\equiv P(A,B,P,t_{0}) = P (A) P (B) \\pi (P) P (t_{0}) $$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "P(\\theta) = \n",
    "\\begin{cases}\n",
    "    1/(\\theta_{max}-\\theta_{min}) & \\text{if } \\theta_{min} <\\theta <\\theta_{max} \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "for $\\theta \\in \\lbrace A, B, P, t_{0} \\rbrace$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(params):\n",
    "    A, B, P, t0 = params\n",
    "    # A -> [0.1, 100.0]\n",
    "    # B -> [-10.0, 10.0]\n",
    "    # P -> [0.3, 10.0]\n",
    "    # t0 -> [0.0, 1.0]\n",
    "\n",
    "    if np.abs(B)>10.0 or A<0.1 or A>100.0 or P<0.3 or P>10.0 or t0<0.0 or t0>1.0:\n",
    "        return -np.inf\n",
    "        \n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the likelihood function we assume that the errors are Gaussian and independend. This means the likelihood function is Gaussian (i.e. the distribution is Normal).\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L}(\\theta) \\equiv \\log P(D|A,B,P,t_{0}) = -\\frac{1}{2}\\sum_{n}\\Bigg[ \\frac{\\big(\\mathcal{M}(A,B,P,t_{0})-D\\big)^{2}}{\\sigma^{2}} +\\log\\big(2\\pi\\sigma^{2}\\big)\\Bigg]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_like(params):\n",
    "    # compute for each t point, where it should lie in y\n",
    "    y_model = sine_model(params, t)\n",
    "    # compute likelihood\n",
    "    loglike = -0.5 * (((y_model - y) / yerr)**2).sum()\n",
    "\n",
    "    return loglike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very simple to combine the two\n",
    "\n",
    "$$\\log \\mathcal{P}(\\theta) \\propto \\log \\mathcal{L}(\\theta) + \\log \\pi (\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_post(params):\n",
    "    lp = log_prior(params)\n",
    "    if ~np.isfinite(lp):\n",
    "        return -np.inf \n",
    "    return lp + log_like(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum a Posteriori (MAP) estimate\n",
    "\n",
    "We can maximise the log-posterior (or equivalently minimize the negative log-posterior) in order to find the MAP estimate. To do this we will use the scipy.optimize.minimize function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Initial guess\n",
    "p_guess = [10.0, 2.0, 4.0, 0.5]\n",
    "\n",
    "\n",
    "# Run the minimisation procedure using the Nelder-Mead method\n",
    "results = minimize(lambda x : -log_post(x), p0, method='Nelder-Mead', options={'maxiter':2000, 'disp':True})\n",
    "\n",
    "print('MAP =', results.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain Monte Carlo with **zeus**\n",
    "\n",
    "Before we run the MCMC we first need to initialise the walkers. We choose to place them close to the MAP estimate. The number of walkers needs to be at least twice the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 4 \n",
    "nwalkers = 10\n",
    "nsteps = 1000\n",
    "\n",
    "start = results.x + 0.001 * np.random.randn(nwalkers, ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now initialise the sampler and run the MCMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = zeus.EnsembleSampler(nwalkers, ndim, log_post)\n",
    "\n",
    "sampler.run_mcmc(start, nsteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We can access the results using the \".get_chain()\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.get_chain()\n",
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good practice to plot the walker trajectories/chains and inspect them to be sure that they have mixed sufficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['A', 'B', 'P', 't0']\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "for i in range(ndim):\n",
    "    plt.subplot(ndim,1,i+1)\n",
    "    plt.plot(samples[:,:,i],alpha=0.6)\n",
    "    plt.ylabel(labels[i], fontsize=14)\n",
    "plt.xlabel('Iteration', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can discard the first half of the chain to make sure that the chain has converged, flatten the walker trajectories into a single one, thin the result, and visualise the 1-D and 2-D marginal posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = sampler.get_chain(flat=True, discard=0.5, thin=7)\n",
    "\n",
    "zeus.cornerplot(chain, labels=labels, truth=params_true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the chains we can compute all sorts of statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean =', np.mean(chain,axis=0))\n",
    "print('standard deviation =', np.std(chain,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Predictive Checks\n",
    "\n",
    "Finally, we can perform some Posterior Predictive Checks to make sure that eveything is as it should be.\n",
    "\n",
    "The distribution that reflects the probability of new data given the current data is the *posterior predictive distribution*:\n",
    "\n",
    "$$\n",
    "P(D_{new}|D) = \\int_{\\Theta} P(D_{new}|\\theta,D)P(\\theta|D)d\\theta\n",
    "$$\n",
    "where $P(D_{new}|\\theta,D) = P(D_{new}|\\theta)$ assuming $D_{new}$ is independent of $D$.\n",
    "\n",
    "The easiest way to sample from the posterior predictive distribution is to use the chains that we have already to generate the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_range = np.linspace(0.0, 5.0, 1000)\n",
    "\n",
    "obs = []\n",
    "\n",
    "for s in chain:\n",
    "    m = sine_model(s, t_range)\n",
    "    obs.append(np.random.normal(m, yerr))\n",
    "\n",
    "obs = np.array(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the $95\\%$ intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.fill_between(t_range, np.percentile(obs,2.5,axis=0),np.percentile(obs,97.5,axis=0),alpha=0.5)\n",
    "plt.plot(t_range, np.mean(obs, axis=0))\n",
    "plt.errorbar(x=t, y=y, yerr=yerr, marker='o', ls=' ', color='red')\n",
    "plt.xlabel('t', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
